{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232b7b61-d535-4d08-99c5-75bb587323c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, size, col, regexp_replace\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ActivityRecognition\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "raw = spark.read.text(\"C:/Users/johnu/Desktop/BigDataProject/WISDM_DATA/WISDM_at_v2.0_raw.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ba2a20-c297-49e7-955e-b73ea540eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = raw.withColumn(\"parts\", split(col(\"value\"), \",\")) \\\n",
    "           .filter(size(col(\"parts\")) == 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea4909f-a393-4f8f-9056-f44fc1884dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean.select(\n",
    "    col(\"parts\").getItem(0).alias(\"user\"),\n",
    "    col(\"parts\").getItem(1).alias(\"activity\"),\n",
    "    col(\"parts\").getItem(2).alias(\"timestamp\"),\n",
    "    col(\"parts\").getItem(3).alias(\"x\"),\n",
    "    col(\"parts\").getItem(4).alias(\"y\"),\n",
    "    regexp_replace(col(\"parts\").getItem(5), \";\", \"\").alias(\"z\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cf81d6-9542-4371-8d99-d67526b88cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "    col(\"user\").cast(\"int\"),\n",
    "    col(\"activity\"),\n",
    "    col(\"timestamp\").cast(\"long\"),\n",
    "    col(\"x\").cast(\"double\"),\n",
    "    col(\"y\").cast(\"double\"),\n",
    "    col(\"z\").cast(\"double\")\n",
    ").dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1cb772f-19ef-4db9-9510-b213700bd278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n",
      "Rows: 2980765\n",
      "+----+--------+-------------+-----------+----------+-----------+\n",
      "|user|activity|timestamp    |x          |y         |z          |\n",
      "+----+--------+-------------+-----------+----------+-----------+\n",
      "|1679|Walking |1370520469556|0.2941316  |-0.6356053|-0.22693644|\n",
      "|1679|Walking |1370520469606|-0.49968776|-0.6044512|-0.22602014|\n",
      "|1679|Walking |1370520469656|-2.1783454 |0.7134906 |0.37201694 |\n",
      "|1679|Walking |1370520469706|-2.7977629 |1.3548992 |-0.27763826|\n",
      "|1679|Walking |1370520469756|-2.1679606 |-1.3277156|-0.5549711 |\n",
      "+----+--------+-------------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(\"Rows:\", df.count())\n",
    "df.show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a75d79a-cf57-4609-972e-d3fe402762cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o128.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:802)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:1020)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:184)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:184)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:194)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:194)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:155)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:239)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:592)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:115)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/johnu/Desktop/BigDataProject/parquet/activity_clean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   2001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 2003\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:263\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    265\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o128.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:802)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:1020)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:184)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:184)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:194)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:194)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:155)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:239)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:592)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:115)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"C:/Users/johnu/Desktop/BigDataProject/parquet/activity_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e373e-af13-4be4-93bf-66152007d06b",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Scalability Benchmark â€” Cores x Sample Fraction\n# ============================================================\n# Measures wall-clock training time of a Random Forest pipeline\n# while varying:\n#   - Spark parallelism : local[1], local[2], local[4]\n#   - Data fraction     : 25 %, 50 %, 75 %, 100 %\n#\n# Each combination gets a fresh SparkSession so that the core\n# count actually takes effect (Spark binds thread count at\n# session creation).  Results are collected in a plain Python\n# list and displayed as a Spark DataFrame at the end.\n# ============================================================\n#\n# IMPORTANT: this cell only defines constants and helpers.\n# The benchmark loop runs in the NEXT cell so you can re-run\n# it independently.\n# ============================================================\n\nimport time, itertools, gc\n\n# -- Experiment grid ------------------------------------------\nCORE_CONFIGS   = [\"local[1]\", \"local[2]\", \"local[4]\"]\nSAMPLE_FRACS   = [0.25, 0.50, 0.75, 1.0]\nCV_FOLDS       = 3\nSEED           = 42\n\nINPUT_PATH = r\"C:/Users/johnu/Desktop/BigDataProject/data/pamap2_features.parquet\"\n\nprint(f\"Configurations : {len(CORE_CONFIGS)} core settings x {len(SAMPLE_FRACS)} fractions \"\n      f\"= {len(CORE_CONFIGS) * len(SAMPLE_FRACS)} runs\")\nprint(f\"CV folds per run: {CV_FOLDS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7e6ee-4721-4f1f-9c90-dd44d6000221",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Benchmark loop\n# ============================================================\n# For each (cores, fraction) pair:\n#   1. Stop any existing SparkSession\n#   2. Create a new session with the target core count\n#   3. Load parquet, sample to the target fraction\n#   4. Build a fresh Pipeline (StringIndexer -> VectorAssembler\n#      -> StandardScaler -> RandomForestClassifier)\n#   5. Run 3-fold CrossValidator and measure wall-clock time\n#   6. Evaluate accuracy + F1 on a held-out test split\n#   7. Tear down the session\n# ============================================================\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import col, isnan, when\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nbench_results = []\n\nfor cores in CORE_CONFIGS:\n    for frac in SAMPLE_FRACS:\n        tag = f\"cores={cores}, frac={frac}\"\n        print(f\"\\n{'-' * 60}\")\n        print(f\"  {tag}\")\n        print(f\"{'-' * 60}\")\n\n        # -- 1. Fresh SparkSession --------------------------------\n        active = SparkSession.getActiveSession()\n        if active is not None:\n            active.stop()\n            gc.collect()\n            time.sleep(3)\n\n        spark_bench = (\n            SparkSession.builder\n            .appName(f\"Bench_{cores}_{frac}\")\n            .master(cores)\n            .config(\"spark.driver.memory\", \"2g\")\n            .config(\"spark.driver.extraJavaOptions\",\n                    \"-XX:+UseG1GC -XX:G1HeapRegionSize=16m\")\n            .config(\"spark.sql.shuffle.partitions\", \"4\")\n            .config(\"spark.ui.enabled\", \"false\")\n            .config(\"spark.python.worker.reuse\", \"true\")\n            .getOrCreate()\n        )\n        spark_bench.sparkContext.setLogLevel(\"ERROR\")\n\n        # -- 2. Load & sample ------------------------------------\n        df_all = spark_bench.read.parquet(INPUT_PATH)\n\n        META = {\"subject_id\", \"activity_id\"}\n        feat_cols = sorted([\n            c for c in df_all.columns\n            if c not in META\n            and isinstance(df_all.schema[c].dataType, DoubleType)\n        ])\n\n        for c in feat_cols:\n            df_all = df_all.withColumn(\n                c, when(isnan(col(c)), 0.0).otherwise(col(c)))\n        df_all = df_all.na.drop(subset=feat_cols)\n\n        if frac < 1.0:\n            df_sampled = df_all.sample(withReplacement=False,\n                                       fraction=frac, seed=SEED)\n        else:\n            df_sampled = df_all\n\n        n_rows = df_sampled.count()\n        train_df, test_df = df_sampled.randomSplit([0.8, 0.2], seed=SEED)\n        train_df.cache()\n        test_df.cache()\n        train_count = train_df.count()\n        test_count  = test_df.count()\n\n        print(f\"  Rows: {n_rows:,}  (train {train_count:,} / test {test_count:,})\")\n\n        # -- 3. Pipeline -----------------------------------------\n        idx = StringIndexer(inputCol=\"activity_id\",\n                            outputCol=\"label\").setHandleInvalid(\"keep\")\n        asm = VectorAssembler(inputCols=feat_cols,\n                              outputCol=\"features_raw\",\n                              handleInvalid=\"skip\")\n        scl = StandardScaler(inputCol=\"features_raw\",\n                             outputCol=\"features\",\n                             withMean=True, withStd=True)\n        rf  = RandomForestClassifier(featuresCol=\"features\",\n                                     labelCol=\"label\",\n                                     numTrees=20,\n                                     maxDepth=5,\n                                     seed=SEED)\n\n        pipe = Pipeline(stages=[idx, asm, scl, rf])\n\n        grid = (ParamGridBuilder()\n                .addGrid(rf.numTrees, [20])\n                .build())\n\n        cv = CrossValidator(\n            estimator=pipe,\n            estimatorParamMaps=grid,\n            evaluator=MulticlassClassificationEvaluator(\n                labelCol=\"label\", metricName=\"f1\"),\n            numFolds=CV_FOLDS,\n            seed=SEED,\n        )\n\n        # -- 4. Train + time -------------------------------------\n        t_start = time.time()\n        cv_model = cv.fit(train_df)\n        train_time = time.time() - t_start\n\n        # -- 5. Evaluate -----------------------------------------\n        preds = cv_model.transform(test_df)\n        acc = MulticlassClassificationEvaluator(\n            labelCol=\"label\", metricName=\"accuracy\"\n        ).evaluate(preds)\n        f1 = MulticlassClassificationEvaluator(\n            labelCol=\"label\", metricName=\"f1\"\n        ).evaluate(preds)\n\n        bench_results.append({\n            \"cores\":       cores,\n            \"fraction\":    frac,\n            \"rows\":        n_rows,\n            \"train_rows\":  train_count,\n            \"train_sec\":   round(train_time, 1),\n            \"accuracy\":    round(acc, 4),\n            \"f1_weighted\": round(f1, 4),\n        })\n\n        print(f\"  Train time : {train_time:.1f}s\")\n        print(f\"  Accuracy   : {acc:.4f}\")\n        print(f\"  F1         : {f1:.4f}\")\n\n        # -- 6. Cleanup ------------------------------------------\n        train_df.unpersist()\n        test_df.unpersist()\n        spark_bench.stop()\n        gc.collect()\n        time.sleep(3)\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"  All {len(bench_results)} benchmark runs complete.\")\nprint(f\"{'=' * 60}\")"
  },
  {
   "cell_type": "code",
   "id": "fw8suqhwuqe",
   "source": "# ============================================================\n# Results table\n# ============================================================\n\n# Restart a lightweight session just to display the table\nspark_display = (\n    SparkSession.builder\n    .appName(\"Bench_Results\")\n    .master(\"local[2]\")\n    .getOrCreate()\n)\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col, round as spark_round\n\ndf_bench = spark_display.createDataFrame([Row(**r) for r in bench_results])\n\ndf_bench = df_bench.select(\n    \"cores\",\n    \"fraction\",\n    \"rows\",\n    \"train_rows\",\n    \"train_sec\",\n    spark_round(\"accuracy\", 4).alias(\"accuracy\"),\n    spark_round(\"f1_weighted\", 4).alias(\"f1_weighted\"),\n)\n\nprint(\"=\" * 80)\nprint(\"  SCALABILITY BENCHMARK -- Random Forest (20 trees, depth 5, 3-fold CV)\")\nprint(\"=\" * 80)\ndf_bench.orderBy(\"cores\", \"fraction\").show(20, truncate=False)\n\n# -- Speedup relative to local[1] at each fraction -----------\nprint(\"=\" * 80)\nprint(\"  SPEEDUP vs local[1]  (time_local1 / time_localN)\")\nprint(\"=\" * 80)\n\nbaseline = {r[\"fraction\"]: r[\"train_sec\"] for r in bench_results\n            if r[\"cores\"] == \"local[1]\"}\n\nspeedup_rows = []\nfor r in bench_results:\n    base_t = baseline.get(r[\"fraction\"], r[\"train_sec\"])\n    speedup_rows.append(Row(\n        cores=r[\"cores\"],\n        fraction=r[\"fraction\"],\n        train_sec=r[\"train_sec\"],\n        speedup=round(base_t / r[\"train_sec\"], 2) if r[\"train_sec\"] > 0 else 0.0,\n    ))\n\ndf_speedup = spark_display.createDataFrame(speedup_rows)\ndf_speedup.orderBy(\"fraction\", \"cores\").show(20, truncate=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fv97jld58z5",
   "source": "# ============================================================\n# Save results to JSON for use in Tableau / report\n# ============================================================\n\nimport json\n\nout_path = r\"C:/Users/johnu/Desktop/BigDataProject/data/scalability_results.json\"\nwith open(out_path, \"w\") as f:\n    json.dump(bench_results, f, indent=2)\n\nprint(f\"Saved {len(bench_results)} benchmark records to {out_path}\")\n\n# Also save as CSV for Tableau import\nimport csv\n\ncsv_path = r\"C:/Users/johnu/Desktop/BigDataProject/data/scalability_results.csv\"\nwith open(csv_path, \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=bench_results[0].keys())\n    writer.writeheader()\n    writer.writerows(bench_results)\n\nprint(f\"Saved CSV to {csv_path}\")\nspark_display.stop()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}