{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3364a696-9346-43d6-b902-bc7688899e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:31.395559Z",
     "iopub.status.busy": "2026-02-19T22:10:31.395559Z",
     "iopub.status.idle": "2026-02-19T22:10:37.817703Z",
     "shell.execute_reply": "2026-02-19T22:10:37.817703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version : 4.1.1\n",
      "Parallelism   : 16\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PAMAP2 Data Ingestion & Preprocessing Pipeline — PySpark\n",
    "# ============================================================\n",
    "# 1. Load all .dat files (Protocol + Optional) with typed schema\n",
    "# 2. Tag rows with subject_id and session_type\n",
    "# 3. Interpolate missing heart rate (forward-fill via Window)\n",
    "# 4. Drop invalid / transient rows\n",
    "# 5. Normalize numeric sensor columns (min-max per column)\n",
    "# 6. Save as Parquet partitioned by subject_id\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, DoubleType, IntegerType, StringType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, input_file_name, regexp_extract,\n",
    "    last, min as spark_min, max as spark_max,\n",
    "    isnan, when, count,\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PAMAP2_Ingestion\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark version : {spark.version}\")\n",
    "print(f\"Parallelism   : {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "n9cz93bgd9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:37.820709Z",
     "iopub.status.busy": "2026-02-19T22:10:37.819709Z",
     "iopub.status.idle": "2026-02-19T22:10:37.829866Z",
     "shell.execute_reply": "2026-02-19T22:10:37.829866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema field count: 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('timestamp', DoubleType(), False), StructField('activity_id', IntegerType(), False), StructField('heart_rate', DoubleType(), True), StructField('hand_temperature', DoubleType(), True), StructField('hand_acc_16g_x', DoubleType(), True), StructField('hand_acc_16g_y', DoubleType(), True), StructField('hand_acc_16g_z', DoubleType(), True), StructField('hand_acc_6g_x', DoubleType(), True), StructField('hand_acc_6g_y', DoubleType(), True), StructField('hand_acc_6g_z', DoubleType(), True), StructField('hand_gyro_x', DoubleType(), True), StructField('hand_gyro_y', DoubleType(), True), StructField('hand_gyro_z', DoubleType(), True), StructField('hand_mag_x', DoubleType(), True), StructField('hand_mag_y', DoubleType(), True), StructField('hand_mag_z', DoubleType(), True), StructField('hand_orient_0', DoubleType(), True), StructField('hand_orient_1', DoubleType(), True), StructField('hand_orient_2', DoubleType(), True), StructField('hand_orient_3', DoubleType(), True), StructField('chest_temperature', DoubleType(), True), StructField('chest_acc_16g_x', DoubleType(), True), StructField('chest_acc_16g_y', DoubleType(), True), StructField('chest_acc_16g_z', DoubleType(), True), StructField('chest_acc_6g_x', DoubleType(), True), StructField('chest_acc_6g_y', DoubleType(), True), StructField('chest_acc_6g_z', DoubleType(), True), StructField('chest_gyro_x', DoubleType(), True), StructField('chest_gyro_y', DoubleType(), True), StructField('chest_gyro_z', DoubleType(), True), StructField('chest_mag_x', DoubleType(), True), StructField('chest_mag_y', DoubleType(), True), StructField('chest_mag_z', DoubleType(), True), StructField('chest_orient_0', DoubleType(), True), StructField('chest_orient_1', DoubleType(), True), StructField('chest_orient_2', DoubleType(), True), StructField('chest_orient_3', DoubleType(), True), StructField('ankle_temperature', DoubleType(), True), StructField('ankle_acc_16g_x', DoubleType(), True), StructField('ankle_acc_16g_y', DoubleType(), True), StructField('ankle_acc_16g_z', DoubleType(), True), StructField('ankle_acc_6g_x', DoubleType(), True), StructField('ankle_acc_6g_y', DoubleType(), True), StructField('ankle_acc_6g_z', DoubleType(), True), StructField('ankle_gyro_x', DoubleType(), True), StructField('ankle_gyro_y', DoubleType(), True), StructField('ankle_gyro_z', DoubleType(), True), StructField('ankle_mag_x', DoubleType(), True), StructField('ankle_mag_y', DoubleType(), True), StructField('ankle_mag_z', DoubleType(), True), StructField('ankle_orient_0', DoubleType(), True), StructField('ankle_orient_1', DoubleType(), True), StructField('ankle_orient_2', DoubleType(), True), StructField('ankle_orient_3', DoubleType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Define the 54-column schema matching the PAMAP2 readme\n",
    "# ============================================================\n",
    "# Columns per the spec:\n",
    "#   1  timestamp (s)\n",
    "#   2  activityID\n",
    "#   3  heart_rate (bpm)\n",
    "#   4-20   IMU_hand   (17 cols)\n",
    "#   21-37  IMU_chest  (17 cols)\n",
    "#   38-54  IMU_ankle  (17 cols)\n",
    "#\n",
    "# Each IMU block (17 cols):\n",
    "#   temperature,\n",
    "#   acc_16g_x, acc_16g_y, acc_16g_z,\n",
    "#   acc_6g_x,  acc_6g_y,  acc_6g_z,\n",
    "#   gyro_x,    gyro_y,    gyro_z,\n",
    "#   mag_x,     mag_y,     mag_z,\n",
    "#   orient_0,  orient_1,  orient_2, orient_3\n",
    "# ============================================================\n",
    "\n",
    "IMU_SUFFIXES = [\n",
    "    \"temperature\",\n",
    "    \"acc_16g_x\", \"acc_16g_y\", \"acc_16g_z\",\n",
    "    \"acc_6g_x\",  \"acc_6g_y\",  \"acc_6g_z\",\n",
    "    \"gyro_x\",    \"gyro_y\",    \"gyro_z\",\n",
    "    \"mag_x\",     \"mag_y\",     \"mag_z\",\n",
    "    \"orient_0\",  \"orient_1\",  \"orient_2\", \"orient_3\",\n",
    "]\n",
    "\n",
    "def build_imu_fields(location: str) -> list:\n",
    "    \"\"\"Generate 17 StructFields for one IMU placement.\"\"\"\n",
    "    return [\n",
    "        StructField(f\"{location}_{s}\", DoubleType(), True)\n",
    "        for s in IMU_SUFFIXES\n",
    "    ]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"timestamp\",   DoubleType(),  False),\n",
    "        StructField(\"activity_id\", IntegerType(), False),\n",
    "        StructField(\"heart_rate\",  DoubleType(),  True),\n",
    "    ]\n",
    "    + build_imu_fields(\"hand\")\n",
    "    + build_imu_fields(\"chest\")\n",
    "    + build_imu_fields(\"ankle\")\n",
    ")\n",
    "\n",
    "print(f\"Schema field count: {len(schema.fields)}\")   # expect 54\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4tlmgklb6w",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:37.832873Z",
     "iopub.status.busy": "2026-02-19T22:10:37.832873Z",
     "iopub.status.idle": "2026-02-19T22:10:44.238476Z",
     "shell.execute_reply": "2026-02-19T22:10:44.238476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  protocol: found 9 files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  optional: found 5 files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Protocol rows : 2,872,533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optional rows : 977,972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows : 3,850,505\n",
      "Columns       : 56 (timestamp, activity_id, heart_rate, hand_temperature, hand_acc_16g_x ... ankle_orient_3, subject_id, session_type)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Load all .dat files and tag with subject_id / session_type\n",
    "# ============================================================\n",
    "# NOTE: On Windows, Hadoop's native glob can fail with\n",
    "# UnsatisfiedLinkError on NativeIO.  We avoid this entirely by\n",
    "# listing files with Python's os module and passing explicit\n",
    "# paths to spark.read.csv().\n",
    "# ============================================================\n",
    "\n",
    "import os, re\n",
    "\n",
    "DATA_ROOT = r\"C:/Users/johnu/Downloads/pamap2+physical+activity+monitoring/PAMAP2_Dataset\"\n",
    "\n",
    "protocol_path = os.path.join(DATA_ROOT, \"Protocol\")\n",
    "optional_path = os.path.join(DATA_ROOT, \"Optional\")\n",
    "\n",
    "def list_dat_files(folder: str) -> list:\n",
    "    \"\"\"Return sorted list of absolute .dat file paths in folder.\"\"\"\n",
    "    return sorted([\n",
    "        os.path.join(folder, f).replace(\"\\\\\", \"/\")\n",
    "        for f in os.listdir(folder)\n",
    "        if f.endswith(\".dat\")\n",
    "    ])\n",
    "\n",
    "def load_session(folder_path: str, session_type: str):\n",
    "    \"\"\"\n",
    "    Read every .dat file inside *folder_path* with the predefined\n",
    "    schema, then add subject_id (int) and session_type (str).\n",
    "    \"\"\"\n",
    "    file_list = list_dat_files(folder_path)\n",
    "    print(f\"  {session_type}: found {len(file_list)} files\")\n",
    "\n",
    "    # Pass explicit file list — no Hadoop glob needed\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(schema)\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"delimiter\", \" \")\n",
    "        .option(\"nanValue\", \"NaN\")\n",
    "        .csv(file_list)\n",
    "    )\n",
    "\n",
    "    # Extract subject id from the file path  (e.g. \"subject101\")\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"_path\", input_file_name())\n",
    "        .withColumn(\n",
    "            \"subject_id\",\n",
    "            regexp_extract(\"_path\", r\"subject(\\d+)\", 1).cast(IntegerType()),\n",
    "        )\n",
    "        .withColumn(\"session_type\", lit(session_type))\n",
    "        .drop(\"_path\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_protocol = load_session(protocol_path, \"protocol\")\n",
    "df_optional = load_session(optional_path, \"optional\")\n",
    "\n",
    "# Union both sessions into one DataFrame\n",
    "df_raw = df_protocol.unionByName(df_optional)\n",
    "\n",
    "print(f\"\\nProtocol rows : {df_protocol.count():,}\")\n",
    "print(f\"Optional rows : {df_optional.count():,}\")\n",
    "print(f\"Combined rows : {df_raw.count():,}\")\n",
    "print(f\"Columns       : {len(df_raw.columns)} ({', '.join(df_raw.columns[:5])} ... {', '.join(df_raw.columns[-3:])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81hjns923zk",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:44.241482Z",
     "iopub.status.busy": "2026-02-19T22:10:44.240484Z",
     "iopub.status.idle": "2026-02-19T22:10:50.182523Z",
     "shell.execute_reply": "2026-02-19T22:10:50.182523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Activity distribution ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|activity_id|count  |\n",
      "+-----------+-------+\n",
      "|0          |1125552|\n",
      "|1          |192523 |\n",
      "|2          |185188 |\n",
      "|3          |189931 |\n",
      "|4          |238761 |\n",
      "|5          |98199  |\n",
      "|6          |164600 |\n",
      "|7          |188107 |\n",
      "|9          |83646  |\n",
      "|10         |309935 |\n",
      "|11         |54519  |\n",
      "|12         |117216 |\n",
      "|13         |104944 |\n",
      "|16         |175353 |\n",
      "|17         |238690 |\n",
      "|18         |99878  |\n",
      "|19         |187188 |\n",
      "|20         |46915  |\n",
      "|24         |49360  |\n",
      "+-----------+-------+\n",
      "\n",
      "=== Null / NaN counts for key columns ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----------------+-----------------+\n",
      "|heart_rate|hand_temperature|chest_temperature|ankle_temperature|\n",
      "+----------+----------------+-----------------+-----------------+\n",
      "|0         |0               |0                |0                |\n",
      "+----------+----------------+-----------------+-----------------+\n",
      "\n",
      "=== Subjects × session types ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+\n",
      "|subject_id|session_type| count|\n",
      "+----------+------------+------+\n",
      "|       101|    optional|319352|\n",
      "|       101|    protocol|376417|\n",
      "|       102|    protocol|447000|\n",
      "|       103|    protocol|252833|\n",
      "|       104|    protocol|329576|\n",
      "|       105|    optional|154773|\n",
      "|       105|    protocol|374783|\n",
      "|       106|    optional|129963|\n",
      "|       106|    protocol|361817|\n",
      "|       107|    protocol|313599|\n",
      "|       108|    optional|180412|\n",
      "|       108|    protocol|408031|\n",
      "|       109|    optional|193472|\n",
      "|       109|    protocol|  8477|\n",
      "+----------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. Quick data-quality audit before preprocessing\n",
    "# ============================================================\n",
    "\n",
    "# 3a. Activity class distribution (including transient id=0)\n",
    "print(\"=== Activity distribution ===\")\n",
    "df_raw.groupBy(\"activity_id\").count().orderBy(\"activity_id\").show(25, truncate=False)\n",
    "\n",
    "# 3b. Null counts per column (first 10 + heart_rate)\n",
    "print(\"=== Null / NaN counts for key columns ===\")\n",
    "null_exprs = [\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in [\"heart_rate\", \"hand_temperature\", \"chest_temperature\", \"ankle_temperature\"]\n",
    "]\n",
    "df_raw.select(null_exprs).show(truncate=False)\n",
    "\n",
    "# 3c. Subjects present\n",
    "print(\"=== Subjects × session types ===\")\n",
    "df_raw.groupBy(\"subject_id\", \"session_type\").count().orderBy(\"subject_id\", \"session_type\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hrnczemwjis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:50.185534Z",
     "iopub.status.busy": "2026-02-19T22:10:50.185534Z",
     "iopub.status.idle": "2026-02-19T22:10:53.397302Z",
     "shell.execute_reply": "2026-02-19T22:10:53.397302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing transient (id=0): 2,724,953 rows\n",
      "Dropped 12 invalid orientation columns → 44 columns remain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping full-sensor-dropout rows: 2,724,953 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Preprocessing — Step A: Drop invalid rows\n",
    "# ============================================================\n",
    "# - Remove transient activities (activity_id == 0): these are\n",
    "#   unlabelled transitions the readme says to discard.\n",
    "# - Drop the 4 orientation columns per IMU (invalid per readme).\n",
    "# - Drop rows where ALL three IMU acceleration columns are null\n",
    "#   (indicates complete sensor dropout for that timestamp).\n",
    "# ============================================================\n",
    "\n",
    "# 4a. Remove transient rows\n",
    "df_clean = df_raw.filter(col(\"activity_id\") != 0)\n",
    "print(f\"After removing transient (id=0): {df_clean.count():,} rows\")\n",
    "\n",
    "# 4b. Drop the 12 orientation columns (marked invalid in readme)\n",
    "orient_cols = [\n",
    "    f\"{loc}_orient_{i}\"\n",
    "    for loc in (\"hand\", \"chest\", \"ankle\")\n",
    "    for i in range(4)\n",
    "]\n",
    "df_clean = df_clean.drop(*orient_cols)\n",
    "print(f\"Dropped {len(orient_cols)} invalid orientation columns → {len(df_clean.columns)} columns remain\")\n",
    "\n",
    "# 4c. Drop rows where all three key accelerometers are null\n",
    "#     (complete sensor failure — not recoverable)\n",
    "df_clean = df_clean.filter(\n",
    "    col(\"hand_acc_16g_x\").isNotNull()\n",
    "    | col(\"chest_acc_16g_x\").isNotNull()\n",
    "    | col(\"ankle_acc_16g_x\").isNotNull()\n",
    ")\n",
    "print(f\"After dropping full-sensor-dropout rows: {df_clean.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5m1ut10opwj",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:53.400309Z",
     "iopub.status.busy": "2026-02-19T22:10:53.400309Z",
     "iopub.status.idle": "2026-02-19T22:10:58.059381Z",
     "shell.execute_reply": "2026-02-19T22:10:58.059381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart rate nulls remaining after interpolation: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Preprocessing -- Step B: Interpolate missing heart rate\n",
    "# ============================================================\n",
    "# Heart rate is sampled at ~9 Hz while IMUs run at 100 Hz,\n",
    "# so ~91% of HR values are null by design.  The gap between\n",
    "# consecutive HR readings is ~11 rows.\n",
    "#\n",
    "# Strategy (optimised for local execution):\n",
    "#   1. Forward-fill with a BOUNDED window of 15 rows (covers\n",
    "#      the ~11-row gap with margin).  This avoids the full\n",
    "#      partition sort that an unbounded window requires.\n",
    "#   2. Back-fill leading nulls with the same bounded window.\n",
    "#   3. Any remaining nulls (rare sensor dropouts > 15 rows)\n",
    "#      are filled with the per-subject mean heart rate.\n",
    "# ============================================================\n",
    "from pyspark.sql.functions import first, mean as F_mean\n",
    "\n",
    "HR_FILL_WINDOW = 15   # rows — generous for the ~11-row gap\n",
    "\n",
    "win_fwd = (\n",
    "    Window\n",
    "    .partitionBy(\"subject_id\", \"session_type\")\n",
    "    .orderBy(\"timestamp\")\n",
    "    .rowsBetween(-HR_FILL_WINDOW, 0)\n",
    ")\n",
    "win_bwd = (\n",
    "    Window\n",
    "    .partitionBy(\"subject_id\", \"session_type\")\n",
    "    .orderBy(\"timestamp\")\n",
    "    .rowsBetween(0, HR_FILL_WINDOW)\n",
    ")\n",
    "\n",
    "# Step 1: forward-fill (carry last known HR value forward)\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"heart_rate\",\n",
    "    last(\"heart_rate\", ignorenulls=True).over(win_fwd)\n",
    ")\n",
    "\n",
    "# Step 2: back-fill (cover leading nulls with next known HR)\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"heart_rate\",\n",
    "    first(\"heart_rate\", ignorenulls=True).over(win_bwd)\n",
    ")\n",
    "\n",
    "# Step 3: fill any remaining nulls with per-subject mean HR\n",
    "win_subj = Window.partitionBy(\"subject_id\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"_hr_mean\", F_mean(\"heart_rate\").over(win_subj)\n",
    ").withColumn(\n",
    "    \"heart_rate\",\n",
    "    when(col(\"heart_rate\").isNull(), col(\"_hr_mean\")).otherwise(col(\"heart_rate\"))\n",
    ").drop(\"_hr_mean\")\n",
    "\n",
    "remaining_hr_nulls = df_clean.filter(col(\"heart_rate\").isNull()).count()\n",
    "print(f\"Heart rate nulls remaining after interpolation: {remaining_hr_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "po17fgmip1m",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:10:58.062432Z",
     "iopub.status.busy": "2026-02-19T22:10:58.062432Z",
     "iopub.status.idle": "2026-02-19T22:11:26.685718Z",
     "shell.execute_reply": "2026-02-19T22:11:26.685718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to normalise: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-normalisation stats (sample columns) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+------------+-----------+\n",
      "|summary|heart_rate|hand_acc_16g_x|chest_gyro_x|ankle_mag_z|\n",
      "+-------+----------+--------------+------------+-----------+\n",
      "|    min|       0.0|           0.0|         0.0|        0.0|\n",
      "|    max|       NaN|           NaN|         NaN|        NaN|\n",
      "|  count|   2724953|       2724953|     2724953|    2724953|\n",
      "+-------+----------+--------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Preprocessing — Step C: Min-Max normalisation (0-1)\n",
    "# ============================================================\n",
    "# Normalise every numeric sensor column to [0, 1] using global\n",
    "# min/max.  We exclude metadata columns (timestamp, activity_id,\n",
    "# subject_id, session_type) from scaling.\n",
    "#\n",
    "# Formula:  x_norm = (x - x_min) / (x_max - x_min)\n",
    "# If max == min (constant column), the result is 0.\n",
    "#\n",
    "# IMPORTANT: spark_min / spark_max propagate NaN (unlike null).\n",
    "# We must filter out NaN before computing stats.\n",
    "# ============================================================\n",
    "\n",
    "EXCLUDE_FROM_NORM = {\"timestamp\", \"activity_id\", \"subject_id\", \"session_type\"}\n",
    "\n",
    "sensor_cols = [\n",
    "    c for c in df_clean.columns\n",
    "    if c not in EXCLUDE_FROM_NORM\n",
    "    and df_clean.schema[c].dataType == DoubleType()\n",
    "]\n",
    "print(f\"Columns to normalise: {len(sensor_cols)}\")\n",
    "\n",
    "# Compute global min/max in ONE pass — filter NaN so they\n",
    "# don't propagate into the aggregation result.\n",
    "agg_exprs = []\n",
    "for c in sensor_cols:\n",
    "    safe_col = when(~isnan(col(c)), col(c))   # NaN -> null (skipped by min/max)\n",
    "    agg_exprs.append(spark_min(safe_col).alias(f\"{c}__min\"))\n",
    "    agg_exprs.append(spark_max(safe_col).alias(f\"{c}__max\"))\n",
    "\n",
    "stats_row = df_clean.agg(*agg_exprs).first()\n",
    "\n",
    "# Build the normalisation expressions\n",
    "df_normalised = df_clean\n",
    "for c in sensor_cols:\n",
    "    c_min = stats_row[f\"{c}__min\"]\n",
    "    c_max = stats_row[f\"{c}__max\"]\n",
    "    if c_min is not None and c_max is not None and c_max != c_min:\n",
    "        df_normalised = df_normalised.withColumn(\n",
    "            c,\n",
    "            (col(c) - lit(c_min)) / lit(c_max - c_min),\n",
    "        )\n",
    "    else:\n",
    "        # Constant or all-null column -> set to 0\n",
    "        df_normalised = df_normalised.withColumn(c, lit(0.0))\n",
    "\n",
    "# Quick sanity check on a few columns\n",
    "print(\"\\n=== Post-normalisation stats (sample columns) ===\")\n",
    "check_cols = [\"heart_rate\", \"hand_acc_16g_x\", \"chest_gyro_x\", \"ankle_mag_z\"]\n",
    "df_normalised.select(check_cols).summary(\"min\", \"max\", \"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mnflztsyy8h",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:11:26.688726Z",
     "iopub.status.busy": "2026-02-19T22:11:26.688726Z",
     "iopub.status.idle": "2026-02-19T22:11:57.361446Z",
     "shell.execute_reply": "2026-02-19T22:11:57.358435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet rows   : 2,724,953\n",
      "Parquet columns: 44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions     : 9 subjects\n",
      "\n",
      "Schema preview:\n",
      "root\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- activity_id: integer (nullable = true)\n",
      " |-- heart_rate: double (nullable = true)\n",
      " |-- hand_temperature: double (nullable = true)\n",
      " |-- hand_acc_16g_x: double (nullable = true)\n",
      " |-- hand_acc_16g_y: double (nullable = true)\n",
      " |-- hand_acc_16g_z: double (nullable = true)\n",
      " |-- hand_acc_6g_x: double (nullable = true)\n",
      " |-- hand_acc_6g_y: double (nullable = true)\n",
      " |-- hand_acc_6g_z: double (nullable = true)\n",
      " |-- hand_gyro_x: double (nullable = true)\n",
      " |-- hand_gyro_y: double (nullable = true)\n",
      " |-- hand_gyro_z: double (nullable = true)\n",
      " |-- hand_mag_x: double (nullable = true)\n",
      " |-- hand_mag_y: double (nullable = true)\n",
      " |-- hand_mag_z: double (nullable = true)\n",
      " |-- chest_temperature: double (nullable = true)\n",
      " |-- chest_acc_16g_x: double (nullable = true)\n",
      " |-- chest_acc_16g_y: double (nullable = true)\n",
      " |-- chest_acc_16g_z: double (nullable = true)\n",
      " |-- chest_acc_6g_x: double (nullable = true)\n",
      " |-- chest_acc_6g_y: double (nullable = true)\n",
      " |-- chest_acc_6g_z: double (nullable = true)\n",
      " |-- chest_gyro_x: double (nullable = true)\n",
      " |-- chest_gyro_y: double (nullable = true)\n",
      " |-- chest_gyro_z: double (nullable = true)\n",
      " |-- chest_mag_x: double (nullable = true)\n",
      " |-- chest_mag_y: double (nullable = true)\n",
      " |-- chest_mag_z: double (nullable = true)\n",
      " |-- ankle_temperature: double (nullable = true)\n",
      " |-- ankle_acc_16g_x: double (nullable = true)\n",
      " |-- ankle_acc_16g_y: double (nullable = true)\n",
      " |-- ankle_acc_16g_z: double (nullable = true)\n",
      " |-- ankle_acc_6g_x: double (nullable = true)\n",
      " |-- ankle_acc_6g_y: double (nullable = true)\n",
      " |-- ankle_acc_6g_z: double (nullable = true)\n",
      " |-- ankle_gyro_x: double (nullable = true)\n",
      " |-- ankle_gyro_y: double (nullable = true)\n",
      " |-- ankle_gyro_z: double (nullable = true)\n",
      " |-- ankle_mag_x: double (nullable = true)\n",
      " |-- ankle_mag_y: double (nullable = true)\n",
      " |-- ankle_mag_z: double (nullable = true)\n",
      " |-- session_type: string (nullable = true)\n",
      " |-- subject_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Save as Parquet — partitioned by subject_id\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_PATH = r\"C:/Users/johnu/Desktop/BigDataProject/data/pamap2_clean.parquet\"\n",
    "\n",
    "(\n",
    "    df_normalised\n",
    "    .repartition(\"subject_id\")          # one partition file per subject\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"subject_id\")\n",
    "    .parquet(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "# Verify the write\n",
    "df_verify = spark.read.parquet(OUTPUT_PATH)\n",
    "print(f\"Parquet rows   : {df_verify.count():,}\")\n",
    "print(f\"Parquet columns: {len(df_verify.columns)}\")\n",
    "print(f\"Partitions     : {df_verify.select('subject_id').distinct().count()} subjects\")\n",
    "print(f\"\\nSchema preview:\")\n",
    "df_verify.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
