{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80d0739-8d24-4eca-9d39-4778b6ff5da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:08.528263Z",
     "iopub.status.busy": "2026-02-19T22:29:08.528263Z",
     "iopub.status.idle": "2026-02-19T22:29:19.654975Z",
     "shell.execute_reply": "2026-02-19T22:29:19.654975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,724,953 rows  ×  44 columns\n",
      "root\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- activity_id: integer (nullable = true)\n",
      " |-- heart_rate: double (nullable = true)\n",
      " |-- hand_temperature: double (nullable = true)\n",
      " |-- hand_acc_16g_x: double (nullable = true)\n",
      " |-- hand_acc_16g_y: double (nullable = true)\n",
      " |-- hand_acc_16g_z: double (nullable = true)\n",
      " |-- hand_acc_6g_x: double (nullable = true)\n",
      " |-- hand_acc_6g_y: double (nullable = true)\n",
      " |-- hand_acc_6g_z: double (nullable = true)\n",
      " |-- hand_gyro_x: double (nullable = true)\n",
      " |-- hand_gyro_y: double (nullable = true)\n",
      " |-- hand_gyro_z: double (nullable = true)\n",
      " |-- hand_mag_x: double (nullable = true)\n",
      " |-- hand_mag_y: double (nullable = true)\n",
      " |-- hand_mag_z: double (nullable = true)\n",
      " |-- chest_temperature: double (nullable = true)\n",
      " |-- chest_acc_16g_x: double (nullable = true)\n",
      " |-- chest_acc_16g_y: double (nullable = true)\n",
      " |-- chest_acc_16g_z: double (nullable = true)\n",
      " |-- chest_acc_6g_x: double (nullable = true)\n",
      " |-- chest_acc_6g_y: double (nullable = true)\n",
      " |-- chest_acc_6g_z: double (nullable = true)\n",
      " |-- chest_gyro_x: double (nullable = true)\n",
      " |-- chest_gyro_y: double (nullable = true)\n",
      " |-- chest_gyro_z: double (nullable = true)\n",
      " |-- chest_mag_x: double (nullable = true)\n",
      " |-- chest_mag_y: double (nullable = true)\n",
      " |-- chest_mag_z: double (nullable = true)\n",
      " |-- ankle_temperature: double (nullable = true)\n",
      " |-- ankle_acc_16g_x: double (nullable = true)\n",
      " |-- ankle_acc_16g_y: double (nullable = true)\n",
      " |-- ankle_acc_16g_z: double (nullable = true)\n",
      " |-- ankle_acc_6g_x: double (nullable = true)\n",
      " |-- ankle_acc_6g_y: double (nullable = true)\n",
      " |-- ankle_acc_6g_z: double (nullable = true)\n",
      " |-- ankle_gyro_x: double (nullable = true)\n",
      " |-- ankle_gyro_y: double (nullable = true)\n",
      " |-- ankle_gyro_z: double (nullable = true)\n",
      " |-- ankle_mag_x: double (nullable = true)\n",
      " |-- ankle_mag_y: double (nullable = true)\n",
      " |-- ankle_mag_z: double (nullable = true)\n",
      " |-- session_type: string (nullable = true)\n",
      " |-- subject_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Sliding-Window Feature Extraction — PySpark\n",
    "# ============================================================\n",
    "# Reads the cleaned PAMAP2 parquet produced by data_ingestion,\n",
    "# segments every (subject, activity) stream into 5-second\n",
    "# windows, and computes per-window statistics:\n",
    "#   - mean, std, min, max   for every sensor column\n",
    "#   - Signal Magnitude Area (SMA) for each triaxial sensor group\n",
    "# Output: one row per window → ready for classification.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, floor, abs as spark_abs,\n",
    "    count,\n",
    "    mean   as F_mean,\n",
    "    stddev as F_stddev,\n",
    "    min    as F_min,\n",
    "    max    as F_max,\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PAMAP2_FeatureEngineering\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ── Load cleaned parquet from ingestion pipeline ─────────────\n",
    "INPUT_PATH  = r\"C:/Users/johnu/Desktop/BigDataProject/data/pamap2_clean.parquet\"\n",
    "\n",
    "df = spark.read.parquet(INPUT_PATH)\n",
    "print(f\"Loaded {df.count():,} rows  ×  {len(df.columns)} columns\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "paenyoitorb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:19.658982Z",
     "iopub.status.busy": "2026-02-19T22:29:19.657981Z",
     "iopub.status.idle": "2026-02-19T22:29:19.666422Z",
     "shell.execute_reply": "2026-02-19T22:29:19.665415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window        : 5.0s  (500 samples @ 100 Hz)\n",
      "Min samples   : 250  (discard shorter windows)\n",
      "Sensor cols   : 40\n",
      "Triaxial SMA  : 12 groups\n",
      "   hand_acc_16g  →  hand_acc_16g_x, hand_acc_16g_y, hand_acc_16g_z\n",
      "   hand_acc_6g  →  hand_acc_6g_x, hand_acc_6g_y, hand_acc_6g_z\n",
      "   hand_gyro  →  hand_gyro_x, hand_gyro_y, hand_gyro_z\n",
      "   hand_mag  →  hand_mag_x, hand_mag_y, hand_mag_z\n",
      "   ... (8 more)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Define column groups and window parameters\n",
    "# ============================================================\n",
    "\n",
    "WINDOW_SEC = 5.0        # window length in seconds\n",
    "SAMPLE_HZ  = 100        # IMU sampling rate\n",
    "EXPECTED_SAMPLES = int(WINDOW_SEC * SAMPLE_HZ)        # 500\n",
    "MIN_SAMPLES      = int(EXPECTED_SAMPLES * 0.5)         # 250 (reject < 50 %-full windows)\n",
    "\n",
    "# ── Sensor columns to aggregate ──────────────────────────────\n",
    "# Everything that is a DoubleType sensor reading (not metadata)\n",
    "META_COLS = {\"timestamp\", \"activity_id\", \"subject_id\", \"session_type\"}\n",
    "\n",
    "sensor_cols = sorted([\n",
    "    c for c in df.columns\n",
    "    if c not in META_COLS\n",
    "    and isinstance(df.schema[c].dataType, DoubleType)\n",
    "])\n",
    "\n",
    "# ── Triaxial groups for SMA ──────────────────────────────────\n",
    "# SMA = (1/n) * Σ (|x| + |y| + |z|)   over the window\n",
    "# One SMA value per triaxial sensor × body location\n",
    "IMU_LOCATIONS = [\"hand\", \"chest\", \"ankle\"]\n",
    "TRIAXIAL_SENSORS = [\"acc_16g\", \"acc_6g\", \"gyro\", \"mag\"]\n",
    "\n",
    "triaxial_groups = [\n",
    "    (f\"{loc}_{sensor}\", f\"{loc}_{sensor}_x\", f\"{loc}_{sensor}_y\", f\"{loc}_{sensor}_z\")\n",
    "    for loc in IMU_LOCATIONS\n",
    "    for sensor in TRIAXIAL_SENSORS\n",
    "]\n",
    "\n",
    "print(f\"Window        : {WINDOW_SEC}s  ({EXPECTED_SAMPLES} samples @ {SAMPLE_HZ} Hz)\")\n",
    "print(f\"Min samples   : {MIN_SAMPLES}  (discard shorter windows)\")\n",
    "print(f\"Sensor cols   : {len(sensor_cols)}\")\n",
    "print(f\"Triaxial SMA  : {len(triaxial_groups)} groups\")\n",
    "for name, x, y, z in triaxial_groups[:4]:\n",
    "    print(f\"   {name}  →  {x}, {y}, {z}\")\n",
    "print(f\"   ... ({len(triaxial_groups) - 4} more)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zsglbn4oy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:19.669421Z",
     "iopub.status.busy": "2026-02-19T22:29:19.668422Z",
     "iopub.status.idle": "2026-02-19T22:29:22.773983Z",
     "shell.execute_reply": "2026-02-19T22:29:22.773983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Windows per activity (sample) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------------+\n",
      "|activity_id|rows  |max_window_id|\n",
      "+-----------+------+-------------+\n",
      "|1          |192523|54           |\n",
      "|2          |185188|57           |\n",
      "|3          |189931|51           |\n",
      "|4          |238761|67           |\n",
      "|5          |98199 |49           |\n",
      "|6          |164600|50           |\n",
      "|7          |188107|59           |\n",
      "|9          |83646 |167          |\n",
      "|10         |309935|221          |\n",
      "|11         |54519 |109          |\n",
      "|12         |117216|156          |\n",
      "|13         |104944|133          |\n",
      "|16         |175353|48           |\n",
      "|17         |238690|75           |\n",
      "|18         |99878 |54           |\n",
      "|19         |187188|108          |\n",
      "|20         |46915 |57           |\n",
      "|24         |49360 |26           |\n",
      "+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Assign each row to a 5-second window\n",
    "# ============================================================\n",
    "# Windows are computed *within* each (subject, activity) segment\n",
    "# so that no window ever straddles two different activities.\n",
    "#\n",
    "#   window_id = floor( (t − t_min) / 5.0 )\n",
    "#\n",
    "# t_min is the earliest timestamp in each segment, computed\n",
    "# once via a Spark Window aggregate and broadcast-joined back.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import min as F_min_win   # alias to avoid clash\n",
    "\n",
    "seg_window = Window.partitionBy(\"subject_id\", \"activity_id\")\n",
    "\n",
    "df_win = (\n",
    "    df\n",
    "    .withColumn(\"_t0\", F_min_win(\"timestamp\").over(seg_window))\n",
    "    .withColumn(\n",
    "        \"window_id\",\n",
    "        floor((col(\"timestamp\") - col(\"_t0\")) / lit(WINDOW_SEC)).cast(\"long\"),\n",
    "    )\n",
    "    .drop(\"_t0\")\n",
    ")\n",
    "\n",
    "# Quick look: how many windows per activity\n",
    "print(\"=== Windows per activity (sample) ===\")\n",
    "(\n",
    "    df_win\n",
    "    .groupBy(\"activity_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"rows\"),\n",
    "        F_max(\"window_id\").alias(\"max_window_id\"),\n",
    "    )\n",
    "    .orderBy(\"activity_id\")\n",
    "    .show(20, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fj64rp61m4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:22.777993Z",
     "iopub.status.busy": "2026-02-19T22:29:22.776990Z",
     "iopub.status.idle": "2026-02-19T22:29:23.341171Z",
     "shell.execute_reply": "2026-02-19T22:29:23.341171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat features (mean/std/min/max) : 160  (40 cols × 4)\n",
      "SMA features                     : 12\n",
      "Total feature columns            : 172\n",
      "Aggregation expressions built    : 173 (incl. sample_count)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. Build aggregation expressions\n",
    "# ============================================================\n",
    "# For every sensor column → mean, std, min, max   (4 features)\n",
    "# For every triaxial group → SMA                  (1 feature)\n",
    "# Plus a sample count per window for quality gating.\n",
    "#\n",
    "# Everything is expressed as a single list of Column objects\n",
    "# so the entire extraction runs in ONE groupBy().agg() pass.\n",
    "# ============================================================\n",
    "\n",
    "agg_exprs = [\n",
    "    count(\"*\").alias(\"sample_count\"),       # for quality filter\n",
    "]\n",
    "\n",
    "# ── 3a. Per-sensor statistics ────────────────────────────────\n",
    "for c in sensor_cols:\n",
    "    agg_exprs.extend([\n",
    "        F_mean(col(c)).alias(f\"{c}_mean\"),\n",
    "        F_stddev(col(c)).alias(f\"{c}_std\"),\n",
    "        F_min(col(c)).alias(f\"{c}_min\"),\n",
    "        F_max(col(c)).alias(f\"{c}_max\"),\n",
    "    ])\n",
    "\n",
    "# ── 3b. Signal Magnitude Area (SMA) per triaxial group ──────\n",
    "# SMA = mean( |x| + |y| + |z| )  over the window\n",
    "for name, x_col, y_col, z_col in triaxial_groups:\n",
    "    agg_exprs.append(\n",
    "        F_mean(\n",
    "            spark_abs(col(x_col)) + spark_abs(col(y_col)) + spark_abs(col(z_col))\n",
    "        ).alias(f\"{name}_sma\")\n",
    "    )\n",
    "\n",
    "stat_features  = len(sensor_cols) * 4\n",
    "sma_features   = len(triaxial_groups)\n",
    "total_features = stat_features + sma_features\n",
    "\n",
    "print(f\"Stat features (mean/std/min/max) : {stat_features}  ({len(sensor_cols)} cols × 4)\")\n",
    "print(f\"SMA features                     : {sma_features}\")\n",
    "print(f\"Total feature columns            : {total_features}\")\n",
    "print(f\"Aggregation expressions built    : {len(agg_exprs)} (incl. sample_count)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "x7xoz12vel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:23.345177Z",
     "iopub.status.busy": "2026-02-19T22:29:23.344179Z",
     "iopub.status.idle": "2026-02-19T22:29:25.748700Z",
     "shell.execute_reply": "2026-02-19T22:29:25.748192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw feature rows   : 5,525\n",
      "Output columns     : 176\n",
      "\n",
      "=== Sample count distribution across windows ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|summary|sample_count|\n",
      "+-------+------------+\n",
      "|    min|           1|\n",
      "|    25%|         500|\n",
      "|    50%|         500|\n",
      "|    75%|         500|\n",
      "|    max|         501|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Execute the windowed aggregation\n",
    "# ============================================================\n",
    "# Group keys: subject_id, activity_id, window_id\n",
    "# → one output row per 5-second window.\n",
    "# ============================================================\n",
    "\n",
    "GROUP_KEYS = [\"subject_id\", \"activity_id\", \"window_id\"]\n",
    "\n",
    "df_features_raw = (\n",
    "    df_win\n",
    "    .groupBy(*GROUP_KEYS)\n",
    "    .agg(*agg_exprs)\n",
    ")\n",
    "\n",
    "print(f\"Raw feature rows   : {df_features_raw.count():,}\")\n",
    "print(f\"Output columns     : {len(df_features_raw.columns)}\")\n",
    "\n",
    "# Show distribution of sample counts (how full are the windows?)\n",
    "print(\"\\n=== Sample count distribution across windows ===\")\n",
    "df_features_raw.select(\"sample_count\").summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "k826txnxf8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:25.754713Z",
     "iopub.status.busy": "2026-02-19T22:29:25.753712Z",
     "iopub.status.idle": "2026-02-19T22:29:28.111503Z",
     "shell.execute_reply": "2026-02-19T22:29:28.110496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows kept    : 5,447\n",
      "Windows dropped : 78  (< 250 samples)\n",
      "Final columns   : 174  (172 features + subject_id + activity_id)\n",
      "\n",
      "=== Activity distribution (windowed) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|activity_id|count|\n",
      "+-----------+-----+\n",
      "|1          |384  |\n",
      "|2          |372  |\n",
      "|3          |379  |\n",
      "|4          |477  |\n",
      "|5          |196  |\n",
      "|6          |328  |\n",
      "|7          |376  |\n",
      "|9          |167  |\n",
      "|10         |620  |\n",
      "|11         |109  |\n",
      "|12         |234  |\n",
      "|13         |210  |\n",
      "|16         |351  |\n",
      "|17         |477  |\n",
      "|18         |200  |\n",
      "|19         |373  |\n",
      "|20         |94   |\n",
      "|24         |100  |\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Quality gate — drop incomplete windows\n",
    "# ============================================================\n",
    "# A full 5 s window at 100 Hz = 500 samples.\n",
    "# Discard any window with < 250 samples (< 50 % full).\n",
    "# These occur at activity boundaries or due to sensor dropout.\n",
    "# Also drop the helper columns (window_id, sample_count).\n",
    "# ============================================================\n",
    "\n",
    "df_features = (\n",
    "    df_features_raw\n",
    "    .filter(col(\"sample_count\") >= MIN_SAMPLES)\n",
    "    .drop(\"window_id\", \"sample_count\")\n",
    ")\n",
    "\n",
    "remaining = df_features.count()\n",
    "dropped   = df_features_raw.count() - remaining\n",
    "\n",
    "print(f\"Windows kept    : {remaining:,}\")\n",
    "print(f\"Windows dropped : {dropped:,}  (< {MIN_SAMPLES} samples)\")\n",
    "print(f\"Final columns   : {len(df_features.columns)}  \"\n",
    "      f\"({len(df_features.columns) - 2} features + subject_id + activity_id)\")\n",
    "\n",
    "# ── Class balance after windowing ────────────────────────────\n",
    "print(\"\\n=== Activity distribution (windowed) ===\")\n",
    "df_features.groupBy(\"activity_id\").count().orderBy(\"activity_id\").show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ywei6do4ila",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:28.114507Z",
     "iopub.status.busy": "2026-02-19T22:29:28.114507Z",
     "iopub.status.idle": "2026-02-19T22:29:30.819563Z",
     "shell.execute_reply": "2026-02-19T22:29:30.819055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample feature values (hand accelerometer 16 g) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------+---------------------+------------------+------------------+-------------------+---------------------+------------------+------------------+------------------+\n",
      "|subject_id|activity_id|hand_acc_16g_x_mean|hand_acc_16g_x_std   |hand_acc_16g_x_min|hand_acc_16g_x_max|hand_acc_16g_y_mean|hand_acc_16g_y_std   |hand_acc_16g_sma  |hand_acc_6g_sma   |hand_gyro_sma     |\n",
      "+----------+-----------+-------------------+---------------------+------------------+------------------+-------------------+---------------------+------------------+------------------+------------------+\n",
      "|103       |3          |0.5844499633268498 |0.004180867909264736 |0.5712149568566967|0.5940995307758121|0.4317561757384165 |0.004511962930591272 |1.4372349472376116|1.5285712471133523|1.413904558684545 |\n",
      "|103       |3          |0.5774536395759716 |0.004354096668627515 |0.5613958172672966|0.5871381161205174|0.42870209322215114|0.005108805774356302 |1.448109255896683 |1.5547332175270807|1.4148720648161242|\n",
      "|103       |3          |0.5631547489861612 |0.00795403858029096  |0.5431156793008061|0.5861933756133869|0.4185047033629615 |0.010909376367116292 |1.4161451957249003|1.4868105317078175|1.415223316947789 |\n",
      "|103       |3          |0.5589438788687976 |0.0028493892606896117|0.5508772535613847|0.5718427887954021|0.4087916004260977 |0.003937371558576856 |1.4075400245976009|1.4698913896493528|1.411827486526362 |\n",
      "|103       |3          |0.5732865621816815 |0.023819605747412688 |0.5475002206082649|0.6250867661897691|0.4127687631745015 |0.008223025483594328 |1.429310249336141 |1.5157907644397064|1.4238957416979128|\n",
      "|103       |3          |0.5593852372785764 |3.5259770339021555E-4|0.5581336540785672|0.5605170674063757|0.4085444910504292 |3.5736352229300754E-4|1.4090177759490856|1.4730665733170996|1.4146069507087256|\n",
      "|103       |3          |NaN                |NaN                  |0.5465257075770307|NaN               |NaN                |NaN                  |NaN               |NaN               |NaN               |\n",
      "|103       |3          |0.6108214273930246 |0.013844610470490345 |0.5955300852123402|0.6271021281983402|0.41641698592835774|0.0026398202026911262|1.4729527767048232|1.6081660547236405|1.407851128374176 |\n",
      "+----------+-----------+-------------------+---------------------+------------------+------------------+-------------------+---------------------+------------------+------------------+------------------+\n",
      "only showing top 8 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Sanity check — peek at a few feature rows\n",
    "# ============================================================\n",
    "\n",
    "sample_cols = (\n",
    "    [\"subject_id\", \"activity_id\"]\n",
    "    + [c for c in df_features.columns if \"hand_acc_16g\" in c][:6]\n",
    "    + [c for c in df_features.columns if \"_sma\" in c][:3]\n",
    ")\n",
    "\n",
    "print(\"=== Sample feature values (hand accelerometer 16 g) ===\")\n",
    "df_features.select(sample_cols).show(8, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "561d8sedzo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:29:30.821566Z",
     "iopub.status.busy": "2026-02-19T22:29:30.821566Z",
     "iopub.status.idle": "2026-02-19T22:29:52.391012Z",
     "shell.execute_reply": "2026-02-19T22:29:52.391012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5,447 windowed feature rows\n",
      "Columns         : 174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions      : 9 subjects\n",
      "\n",
      "Feature schema (first 20 fields):\n",
      "  activity_id                               int\n",
      "  ankle_acc_16g_x_mean                      double\n",
      "  ankle_acc_16g_x_std                       double\n",
      "  ankle_acc_16g_x_min                       double\n",
      "  ankle_acc_16g_x_max                       double\n",
      "  ankle_acc_16g_y_mean                      double\n",
      "  ankle_acc_16g_y_std                       double\n",
      "  ankle_acc_16g_y_min                       double\n",
      "  ankle_acc_16g_y_max                       double\n",
      "  ankle_acc_16g_z_mean                      double\n",
      "  ankle_acc_16g_z_std                       double\n",
      "  ankle_acc_16g_z_min                       double\n",
      "  ankle_acc_16g_z_max                       double\n",
      "  ankle_acc_6g_x_mean                       double\n",
      "  ankle_acc_6g_x_std                        double\n",
      "  ankle_acc_6g_x_min                        double\n",
      "  ankle_acc_6g_x_max                        double\n",
      "  ankle_acc_6g_y_mean                       double\n",
      "  ankle_acc_6g_y_std                        double\n",
      "  ankle_acc_6g_y_min                        double\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Save windowed features as Parquet (partitioned by subject)\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_PATH = r\"C:/Users/johnu/Desktop/BigDataProject/data/pamap2_features.parquet\"\n",
    "\n",
    "(\n",
    "    df_features\n",
    "    .repartition(\"subject_id\")\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"subject_id\")\n",
    "    .parquet(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "# ── Verify ───────────────────────────────────────────────────\n",
    "df_verify = spark.read.parquet(OUTPUT_PATH)\n",
    "print(f\"Saved {df_verify.count():,} windowed feature rows\")\n",
    "print(f\"Columns         : {len(df_verify.columns)}\")\n",
    "print(f\"Partitions      : {df_verify.select('subject_id').distinct().count()} subjects\")\n",
    "print(f\"\\nFeature schema (first 20 fields):\")\n",
    "for f in df_verify.schema.fields[:20]:\n",
    "    print(f\"  {f.name:40s}  {f.dataType.simpleString()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
